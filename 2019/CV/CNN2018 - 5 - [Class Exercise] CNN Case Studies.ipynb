{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y4GgVIEOx63V"
   },
   "source": [
    "![title](https://image.ibb.co/erDntK/logo2018.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Class Exercise] Convolutional Neural Network Case Studies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import BatchNormalization, Dropout, ZeroPadding2D\n",
    "from keras import Model\n",
    "from keras.layers import Input\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'forse', 'ship', 'truck']\n",
    "\n",
    "num_classes = 10\n",
    "input_shape = [28, 28]\n",
    "\n",
    "X_val = X_train[-1000:,:]\n",
    "y_val = y_train[-1000:]\n",
    "\n",
    "X_train = X_train[:-1000, :]\n",
    "y_train = y_train[:-1000]\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_val = X_val.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "mean_image = np.mean(X_train, axis = 0)\n",
    "X_train -= mean_image\n",
    "X_val -= mean_image\n",
    "X_test -= mean_image\n",
    "\n",
    "print('X_train.shape =',X_train.shape)\n",
    "print('X_val.shape   =',X_val.shape)\n",
    "print('X_test.shape  =',X_test.shape)\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val = keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print('y_train.shape =',y_train.shape)\n",
    "print('y_val.shape   =',y_val.shape)\n",
    "print('y_test.shape  =',y_test.shape)\n",
    "\n",
    "data = (X_train, y_train, X_val, y_val, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuralnet():    \n",
    "    return Sequential([\n",
    "        Flatten(input_shape=[32, 32, 3]),\n",
    "        Dense(200, activation = 'relu'),\n",
    "        Dense(120, activation = 'relu'),\n",
    "        Dense(84, activation = 'relu'),\n",
    "        Dense(num_classes, activation = 'softmax')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_model(model_name, data, epochs = 2, batch_size = 256 ):\n",
    "    \n",
    "    myModel = model_name()\n",
    "    myModel.summary()\n",
    "    myModel.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = data\n",
    "    \n",
    "    myModel.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, \n",
    "                validation_data=(X_val, y_val), verbose=2)\n",
    "    \n",
    "    scores = myModel.evaluate(X_train, y_train, verbose=2)\n",
    "    print('Model Train',myModel.metrics_names[1],': %.2f%%' %(scores[1] * 100))\n",
    "\n",
    "    scores = myModel.evaluate(X_val, y_val, verbose=2)\n",
    "    print('Model Val',myModel.metrics_names[1],': %.2f%%' %(scores[1] * 100))\n",
    "    \n",
    "    y_pred = myModel.predict(X_test)\n",
    "    print('Accuracy = %.2f%%' % (accuracy_score(y_test.argmax(1), y_pred.argmax(1))*100))\n",
    "    \n",
    "    return myModel, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "myModel, y_pred = try_model(neuralnet, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# LeNet-5 [2012]\n",
    "\n",
    "![LeNet5](https://www.researchgate.net/profile/Vladimir_Golovko3/publication/313808170/figure/fig3/AS:552880910618630@1508828489678/Architecture-of-LeNet-5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lenet5():\n",
    "    return Sequential([\n",
    "        Conv2D(6, (5, 5), input_shape=[32, 32, 3], activation='relu'),\n",
    "        AveragePooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(16, (5, 5), activation='relu'),\n",
    "        AveragePooling2D(pool_size=(2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(120, activation = 'relu'),\n",
    "        Dense(84, activation = 'relu'),\n",
    "        Dense(num_classes, activation = 'softmax')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myModel, y_pred = try_model(lenet5, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# AlexNet [2012]\n",
    "\n",
    "The neural network developed by Krizhevsky, Sutskever, and Hinton in 2012 was the coming out party for CNNs in the computer vision community. This was the first time a model performed so well on a historically difficult ImageNet dataset. Utilizing techniques that are still used today, such as data augmentation and dropout, this paper really illustrated the benefits of CNNs and backed them up with record breaking performance in the competition.\n",
    "\n",
    "<img src='https://cdn-images-1.medium.com/max/1600/1*qyc21qM0oxWEuRaj-XJKcw.png' style=\"width:800px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def alexnet():\n",
    "    return Sequential([\n",
    "        # 1\n",
    "        Conv2D(96, (11, 11), strides=4, input_shape=(227,227,3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(3, 3), strides=2),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        # 2\n",
    "        ZeroPadding2D((2, 2)),\n",
    "        Conv2D(256, (5, 5), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(3, 3), strides=2),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        # 3\n",
    "        ZeroPadding2D((1, 1)),\n",
    "        Conv2D(384, (3, 3), activation='relu'),\n",
    "        \n",
    "        # 4\n",
    "        ZeroPadding2D((1, 1)),\n",
    "        Conv2D(384, (3, 3), activation='relu'),\n",
    "        \n",
    "        # 5\n",
    "        ZeroPadding2D((1, 1)),\n",
    "        Conv2D(256, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(3, 3), strides=2),\n",
    "        \n",
    "        # 6\n",
    "        Flatten(),\n",
    "        Dense(4096, activation = 'relu'),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        # 7\n",
    "        Dense(4096, activation = 'relu'),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        # 8\n",
    "        Dense(num_classes, activation = 'softmax')\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myModel = alexnet()\n",
    "myModel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ZF-Net [2013]\n",
    "\n",
    "   ZF Net was not only the winner of the competition in 2013, but also provided great intuition as to the workings on CNNs and illustrated more ways to improve performance. The visualization approach described helps not only to explain the inner workings of CNNs, but also provides insight for improvements to network architectures. The fascinating deconv visualization approach and occlusion experiments make this one of my personal favorite papers.\n",
    "\n",
    "<img src='https://adeshpande3.github.io/assets/zfnet.png' style=\"width:800px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zfnet():\n",
    "    return Sequential([\n",
    "        \n",
    "        # First convolutional Layer (96x7x7)\n",
    "        Conv2D(96, (7,7), input_shape=(224,224,3), strides = 2, activation = \"relu\"),\n",
    "        ZeroPadding2D((1,1)),\n",
    "        MaxPooling2D((3,3), strides=2),\n",
    "        BatchNormalization(),\n",
    "\n",
    "        # Second convolutional Layer (256x5x5)\n",
    "        Conv2D(256, (5,5), strides = 2, activation = \"relu\"),\n",
    "        ZeroPadding2D((1,1)),\n",
    "        MaxPooling2D((3,3), strides=2),\n",
    "        BatchNormalization(),\n",
    "\n",
    "        # 3\n",
    "        ZeroPadding2D((1,1)),\n",
    "        Conv2D(384, (3,3), activation = \"relu\"),\n",
    "\n",
    "        # 4\n",
    "        ZeroPadding2D((1,1)),\n",
    "        Conv2D(384, (3,3), activation = \"relu\"),\n",
    "\n",
    "        # 5\n",
    "        ZeroPadding2D((1,1)),\n",
    "        Conv2D(256, (3,3), activation = \"relu\"),\n",
    "        MaxPooling2D((3,3), strides=2),\n",
    "        \n",
    "        # 6\n",
    "        Flatten(),\n",
    "        Dense(4096, activation=\"relu\"),\n",
    "        Dropout(0.5),\n",
    "\n",
    "        # 7\n",
    "        Dense(4096, activation=\"relu\"),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        # 8\n",
    "        Dense(num_classes, activation = 'softmax')\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "myModel = zfnet()\n",
    "myModel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# VGG-Net [2014]\n",
    "\n",
    "   VGG Net is one of the most influential papers in my mind because it reinforced the notion that convolutional neural networks have to have a deep network of layers in order for this hierarchical representation of visual data to work. Keep it deep. Keep it simple.\n",
    "\n",
    "<img src='http://www.sanko-shoko.net/note.php?img=y1kl' style=\"width:800px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg16():\n",
    "    return Sequential([\n",
    "        \n",
    "        # 1-2\n",
    "        Conv2D(64, 3, input_shape=(224,224,3), activation='relu', padding='same'),\n",
    "        Conv2D(64, 3, activation='relu', padding='same'),\n",
    "        MaxPooling2D(2, 2),\n",
    "\n",
    "        # 3-4\n",
    "        Conv2D(128, 3, activation='relu', padding='same'),\n",
    "        Conv2D(128, 3, activation='relu', padding='same'),\n",
    "        MaxPooling2D(2, 2),\n",
    "\n",
    "        # 5-7\n",
    "        Conv2D(256, 3, activation='relu', padding='same'),\n",
    "        Conv2D(256, 3, activation='relu', padding='same'),\n",
    "        Conv2D(256, 3, activation='relu', padding='same'),\n",
    "        MaxPooling2D(2, 2),\n",
    "        \n",
    "        # 8-10\n",
    "        Conv2D(512, 3, activation='relu', padding='same'),\n",
    "        Conv2D(512, 3, activation='relu', padding='same'),\n",
    "        Conv2D(512, 3, activation='relu', padding='same'),\n",
    "        MaxPooling2D(2, 2),\n",
    "\n",
    "        # 11-13\n",
    "        Conv2D(512, 3, activation='relu', padding='same'),\n",
    "        Conv2D(512, 3, activation='relu', padding='same'),\n",
    "        Conv2D(512, 3, activation='relu', padding='same'),\n",
    "        MaxPooling2D(2, 2),\n",
    "\n",
    "        # 14\n",
    "        Flatten(),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        # 15\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        # 16\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "myModel = vgg16()\n",
    "myModel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IF DON'T ALREADY HAVE A VGG16 MODEL, <br>THIS CODE WILL DOWNLOAD THE MODEL WITH A FAIRLY LARGE SIZE** \n",
    "---\n",
    "\n",
    "**Do not run the last four lines of code if the connection is slow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "model = VGG16(weights=None, include_top=True)\n",
    "\n",
    "# model size ~530MB\n",
    "model = VGG16(weights='imagenet', include_top=True)\n",
    "model.summary()\n",
    "\n",
    "# model size ~57MB\n",
    "model = VGG16(weights='imagenet', include_top=False)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg16_GAP():\n",
    "    return Sequential([\n",
    "        \n",
    "        # 1-2\n",
    "        Conv2D(64, 3, input_shape=(224,224,3), activation='relu', padding='same'),\n",
    "        Conv2D(64, 3, activation='relu', padding='same'),\n",
    "        MaxPooling2D(2, 2),\n",
    "\n",
    "        # 3-4\n",
    "        Conv2D(128, 3, activation='relu', padding='same'),\n",
    "        Conv2D(128, 3, activation='relu', padding='same'),\n",
    "        MaxPooling2D(2, 2),\n",
    "\n",
    "        # 5-7\n",
    "        Conv2D(256, 3, activation='relu', padding='same'),\n",
    "        Conv2D(256, 3, activation='relu', padding='same'),\n",
    "        Conv2D(256, 3, activation='relu', padding='same'),\n",
    "        MaxPooling2D(2, 2),\n",
    "        \n",
    "        # 8-10\n",
    "        Conv2D(512, 3, activation='relu', padding='same'),\n",
    "        Conv2D(512, 3, activation='relu', padding='same'),\n",
    "        Conv2D(512, 3, activation='relu', padding='same'),\n",
    "        MaxPooling2D(2, 2),\n",
    "\n",
    "        # 11-13\n",
    "        Conv2D(512, 3, activation='relu', padding='same'),\n",
    "        Conv2D(512, 3, activation='relu', padding='same'),\n",
    "        Conv2D(512, 3, activation='relu', padding='same'),\n",
    "        MaxPooling2D(2, 2),\n",
    "\n",
    "        # 14\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        # 15\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        # 16\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "myModel = vgg16_GAP()\n",
    "myModel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# GoogLeNet [2014]\n",
    "\n",
    "GoogLeNet was one of the first models that introduced the idea that CNN layers didn’t always have to be stacked up sequentially. Coming up with the Inception module, the authors showed that a creative structuring of layers can lead to improved performance and computationally efficiency. This paper has really set the stage for some amazing architectures that we could see in the coming years.\n",
    "\n",
    "<img src='https://adeshpande3.github.io/assets/GoogleNet.gif' style=\"width:800px;\">\n",
    "<img src='https://adeshpande3.github.io/assets/GoogLeNet.png' style=\"width:800px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def small_googlenet():\n",
    "    \n",
    "    input_img = Input(shape = (32, 32, 3))\n",
    "\n",
    "    tower_1 = Conv2D(64, (1,1), padding='same', activation='relu')(input_img)\n",
    "    tower_1 = Conv2D(64, (3,3), padding='same', activation='relu')(tower_1)\n",
    "\n",
    "    tower_2 = Conv2D(64, (1,1), padding='same', activation='relu')(input_img)\n",
    "    tower_2 = Conv2D(64, (5,5), padding='same', activation='relu')(tower_2)\n",
    "\n",
    "    tower_3 = MaxPooling2D((3,3), strides=(1,1), padding='same')(input_img)\n",
    "    tower_3 = Conv2D(64, (1,1), padding='same', activation='relu')(tower_3)\n",
    "\n",
    "    output = keras.layers.concatenate([tower_1, tower_2, tower_3], axis = 3)\n",
    "    \n",
    "    output = Flatten()(output)\n",
    "    out    = Dense(10, activation='softmax')(output)\n",
    "\n",
    "    model = Model(inputs = input_img, outputs = out)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "myModel = small_googlenet()\n",
    "myModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "model = InceptionV3(weights=None, include_top=True)\n",
    "model.summary()\n",
    "print('Total params:', format(model.count_params(),',d'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Residual Network (ResNet) [2015]\n",
    "\n",
    "3.6% error rate. That itself should be enough to convince you. \n",
    "The ResNet model is the best CNN architecture that we currently have and is a great innovation for the idea of residual learning. \n",
    "\n",
    "<img src='https://adeshpande3.github.io/assets/ResNet.gif' style=\"width:300px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_layer(inputs, num_filters=16, kernel_size=3,\n",
    "                 strides=1, activation='relu', batch_normalization=True,\n",
    "                 conv_first=True):\n",
    "    \n",
    "    conv = Conv2D(num_filters, kernel_size=kernel_size, strides=strides,\n",
    "                  padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))\n",
    "\n",
    "    x = inputs\n",
    "    if conv_first:\n",
    "        x = conv(x)\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "    else:\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "        x = conv(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def resnet_v1():\n",
    "    input_shape = (32, 32, 3)\n",
    "    depth = 20\n",
    "    # Start model definition.\n",
    "    num_filters = 16\n",
    "    num_res_blocks = int((depth - 2) / 6)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = resnet_layer(inputs=inputs)\n",
    "    # Instantiate the stack of residual units\n",
    "    for stack in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            \n",
    "            strides = 1\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                strides = 2  # downsample\n",
    "                \n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters, strides=strides)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters, activation=None)\n",
    "            \n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters, kernel_size=1, strides=strides,\n",
    "                                 activation=None, batch_normalization=False)\n",
    "                \n",
    "                \n",
    "            x = keras.layers.add([x, y])\n",
    "            x = Activation('relu')(x)\n",
    "        num_filters *= 2\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v1 does not use BN after last shortcut connection-ReLU\n",
    "    x = AveragePooling2D(pool_size=8)(x)\n",
    "    y = Flatten()(x)\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # Instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "myModel = resnet_v1()\n",
    "myModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "model = ResNet50(weights=None, include_top=True)\n",
    "# model.summary()\n",
    "print('Total params:', format(model.count_params(),',d'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Going Further: Wide ResNet [2016]\n",
    "\n",
    "\n",
    "<img src='http://i.imgur.com/3b0fw7b.png' style=\"width:500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Going Further: DenseNet [2016]\n",
    "\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/titu1994/DenseNet/master/images/dense_net.JPG' style=\"width:800px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Going Further: ResNet in ResNet [2016]\n",
    "\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/titu1994/Residual-of-Residual-Networks/master/images/resnet%20vs%20ror.JPG' style=\"width:400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Going Further: MobileNet [2017]\n",
    "\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/titu1994/MobileNetworks/master/images/mobilenets.PNG' style=\"width:800px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Going Further: SENet [2017]\n",
    "\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/titu1994/keras-squeeze-excite-network/master/images/squeeze-excite-block.JPG' style=\"width:800px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Going Further: ResNeXt [2017]\n",
    "\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/titu1994/Keras-ResNeXt/master/images/Cardinality.PNG' style=\"width:500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aqhn3S3Ex8me"
   },
   "source": [
    "![footer](https://image.ibb.co/hAHDYK/footer2018.png)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
